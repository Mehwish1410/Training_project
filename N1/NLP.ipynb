{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba9b70bc-0d93-40c8-81bb-52a0ae9da830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#punksentence read sentence word by word\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "#it makes combination of <NN,NP>,exact combinations\n",
    "from nltk import ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7e04a2b-b1e5-48d1-ab4d-668c63ce37a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ANQ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\ANQ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1568ffdd-c225-4aa2-b163-dc14fdba1643",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for saving combinations\n",
    "chunks=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8679ca6b-0940-44e9-b04b-ce9fa7b65a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter your string here: I am taking class in ICG to avail High Impact Training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am taking class in ICG to avail High Impact Training\n"
     ]
    }
   ],
   "source": [
    "#taking inputs here\n",
    "sen=input(\"enter your string here:\")\n",
    "print(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d52fc432-7ec9-4936-be39-539a866cea63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I am taking class in ICG to avail High Impact Training']\n"
     ]
    }
   ],
   "source": [
    "#tokens dy raha hai noun pronoun ko alg alg kr k diff colours dy raha hai\n",
    "custom_tokenizer=PunktSentenceTokenizer(sen)\n",
    "tokenized=custom_tokenizer.tokenize(sen)\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60cd1c16-1c8c-4943-881a-4a4f994f47bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('am', 'VBP'), ('taking', 'VBG'), ('class', 'NN'), ('in', 'IN'), ('ICG', 'NNP'), ('to', 'TO'), ('avail', 'VB'), ('High', 'NNP'), ('Impact', 'NNP'), ('Training', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "#iterate words of sentence\n",
    "for i in tokenized:\n",
    "  words=nltk.word_tokenize(i)   \n",
    "  #pos (parts of speech tagging)\n",
    "  tagged=nltk.pos_tag(words)\n",
    "  print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4caf1e0a-e168-4ef7-b45e-77c27bc0d58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#syntax of chunking to extract specific words\n",
    "ChunkGram=r\"\"\"\n",
    "          Chunk:\n",
    "          {<NN><NNP>|<NN><NN>|<NN><NNS>|<NNP><NNP>|<NNS><NNP>|<NNS><NNS>?}\n",
    "          {<NN|NNP|NNS>?}\n",
    "          {<IN>}\n",
    "          {<VBZ><DT>}\n",
    "          \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8598fc7-99d1-45cf-a140-95c2d52264a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fuction to read or identify extracted words from  chunk \n",
    "ChunkParser=nltk.RegexpParser(ChunkGram)\n",
    "#second method\n",
    "#chunkParser=nltk.ProjectiveDependencyParser(ChunkGram)\n",
    "chunked=ChunkParser.parse(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c76a709-707a-4d54-a1d7-eb833fc7d255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  I/PRP\n",
      "  am/VBP\n",
      "  taking/VBG\n",
      "  (Chunk class/NN)\n",
      "  (Chunk in/IN)\n",
      "  (Chunk ICG/NNP)\n",
      "  to/TO\n",
      "  avail/VB\n",
      "  (Chunk High/NNP Impact/NNP)\n",
      "  (Chunk Training/NNP))\n",
      "hi ['class']\n",
      "(S\n",
      "  I/PRP\n",
      "  am/VBP\n",
      "  taking/VBG\n",
      "  (Chunk class/NN)\n",
      "  (Chunk in/IN)\n",
      "  (Chunk ICG/NNP)\n",
      "  to/TO\n",
      "  avail/VB\n",
      "  (Chunk High/NNP Impact/NNP)\n",
      "  (Chunk Training/NNP))\n",
      "hi ['class', 'in']\n",
      "(S\n",
      "  I/PRP\n",
      "  am/VBP\n",
      "  taking/VBG\n",
      "  (Chunk class/NN)\n",
      "  (Chunk in/IN)\n",
      "  (Chunk ICG/NNP)\n",
      "  to/TO\n",
      "  avail/VB\n",
      "  (Chunk High/NNP Impact/NNP)\n",
      "  (Chunk Training/NNP))\n",
      "hi ['class', 'in', 'ICG']\n",
      "(S\n",
      "  I/PRP\n",
      "  am/VBP\n",
      "  taking/VBG\n",
      "  (Chunk class/NN)\n",
      "  (Chunk in/IN)\n",
      "  (Chunk ICG/NNP)\n",
      "  to/TO\n",
      "  avail/VB\n",
      "  (Chunk High/NNP Impact/NNP)\n",
      "  (Chunk Training/NNP))\n",
      "hi ['class', 'in', 'ICG', 'High', 'High Impact']\n",
      "(S\n",
      "  I/PRP\n",
      "  am/VBP\n",
      "  taking/VBG\n",
      "  (Chunk class/NN)\n",
      "  (Chunk in/IN)\n",
      "  (Chunk ICG/NNP)\n",
      "  to/TO\n",
      "  avail/VB\n",
      "  (Chunk High/NNP Impact/NNP)\n",
      "  (Chunk Training/NNP))\n",
      "hi ['class', 'in', 'ICG', 'High', 'High Impact', 'Training']\n"
     ]
    }
   ],
   "source": [
    "chunks=[]\n",
    "for subtree in chunked.subtrees(filter=lambda t:t.label()=='Chunk'):\n",
    "    chunk = \" \"\n",
    "    for leave in subtree.leaves():\n",
    "      chunk+=leave[0]+' '\n",
    "        #strip function is used to remove labels \n",
    "      chunks.append(chunk.strip())\n",
    "    print(chunked)   \n",
    "    print(\"hi\",chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2f7d912-82b3-406c-8357-4a14ff6d79e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['High']\n"
     ]
    }
   ],
   "source": [
    "finalchunks=[]\n",
    "finalnoun=[]\n",
    "for idx,item in enumerate(chunks[:-1]):\n",
    " if (item in chunks[idx+1]):\n",
    "      finalchunks.append(item)\n",
    "print(finalchunks)    \n",
    "#for elements in  remnoun\n",
    "# if (element!='')\n",
    "# sinlenoun.append(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7aa5990e-4c44-4d14-a0a1-b69accdd1202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final noun phrase extracted from parser ['c', 'l', 'a', ' ', 'i', 'n', ' ', 'I', 'C', 'G', ' ', 'H', 'i', 'g', 'h', ' ', 'I', 'm', 'p', 'a', 'c', 't', ' ', 'T', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ', 'c', 'l', 'a', ' ', 'i', 'n', ' ', 'I', 'C', 'G', ' ', 'H', 'i', 'g', 'h', ' ', 'I', 'm', 'p', 'a', 'c', 't', ' ', 'T', 'r', 'a', 'i', 'n', 'i', 'n', 'g', ' ']\n"
     ]
    }
   ],
   "source": [
    "for i in chunks:\n",
    "     if i not in finalchunks:\n",
    "         finalnoun.append(i)\n",
    "print(\"final noun phrase extracted from parser\",finalnoun)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38d19b63-7e19-40ad-baab-f25be67da299",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59db2006-762f-4ae3-bed7-a904b356bc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonyms: {'passion', 'have_sex', 'make_out', 'do_it', 'jazz', 'make_love', 'screw', 'dearest', 'enjoy', 'get_laid', 'lovemaking', 'dear', 'sleep_together', 'hump', 'sleep_with', 'making_love', 'be_intimate', 'get_it_on', 'beloved', 'have_intercourse', 'roll_in_the_hay', 'bonk', 'lie_with', 'sexual_love', 'fuck', 'eff', 'have_it_off', 'have_a_go_at_it', 'bang', 'have_it_away', 'honey', 'love', 'erotic_love', 'love_life', 'know', 'bed'}\n",
      "Antonyms: {'hate'}\n"
     ]
    }
   ],
   "source": [
    "#SYNONYMS\n",
    "synonyms=[]\n",
    "for syn in wordnet.synsets(\"love\"):\n",
    "    for i in syn.lemmas():\n",
    "        synonyms.append(i.name())\n",
    "print(\"Synonyms:\",set(synonyms))\n",
    "\n",
    "#ANTONYMS\n",
    "antonyms=[]\n",
    "for syn in wordnet.synsets(\"love\"):\n",
    "    for i in syn.lemmas():\n",
    "        if i.antonyms():\n",
    "             antonyms.append(i.antonyms()[0].name())\n",
    "print(\"Antonyms:\",set(antonyms))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
