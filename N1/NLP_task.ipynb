{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc32f39d-bf36-408d-98ec-b5323e4bbf70",
   "metadata": {},
   "source": [
    "### Task: 01\n",
    "  #### Identify if two sentences are similar or not\n",
    "  * Input two sentences(sen1, sen2)\n",
    "  * Tokenize,create chunks:single chunks->save in list1 and list2 consider these two lists as datasets.\n",
    "  * Find semantic similarity(WordNet):\n",
    "     * Formula: wu-palmer similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d08d4101-4b4a-440e-933b-b621cc2e9df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ANQ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bb34976-a64f-4b11-9efd-da1f8a7513d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello.n.01\n",
      "selling.n.01\n"
     ]
    }
   ],
   "source": [
    "syn1 = wordnet.synsets('Hello')[0]\n",
    "syn2 = wordnet.synsets('Selling')[0]\n",
    "print(syn1.name())\n",
    "print(syn2.name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a91e5842-b5df-4a1b-83e6-90d024cfc6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26666666666666666\n"
     ]
    }
   ],
   "source": [
    "sim = syn1.wup_similarity(syn2)\n",
    "print(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "62df4e85-4383-45ec-8a7a-2b28f7ad8be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your string here: Istanbul is a capital of Turkey\n",
      "Enter your string here: Islamabad is a capital of Pakistan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Istanbul is a capital of Turkey\n",
      "Islamabad is a capital of Pakistan\n"
     ]
    }
   ],
   "source": [
    "sen1 =input(\"Enter your string here:\")\n",
    "sen2 =input(\"Enter your string here:\")\n",
    "print(sen1)\n",
    "print(sen2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "facad05a-5dbb-4d9d-be66-7d71f9c8089f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Istanbul', 'is', 'a', 'capital', 'of', 'Turkey']\n",
      "['Islamabad', 'is', 'a', 'capital', 'of', 'Pakistan']\n"
     ]
    }
   ],
   "source": [
    "token_sen1=word_tokenize(sen1)\n",
    "token_sen2=word_tokenize(sen2)\n",
    "print(token_sen1)\n",
    "print(token_sen2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f28506dc-ca67-40e3-adca-ba2949fac1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Istanbul', 'NN')]\n",
      "[('is', 'VBZ')]\n",
      "[('a', 'DT')]\n",
      "[('capital', 'NN')]\n",
      "[('of', 'IN')]\n",
      "[('Turkey', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "for i in token_sen1:\n",
    "  word1=nltk.word_tokenize(i)   \n",
    "  tagged1=nltk.pos_tag(word1)\n",
    "  print(tagged1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "56565dbc-c974-40b0-9b6a-cc9667be30b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Islamabad', 'NN')]\n",
      "[('is', 'VBZ')]\n",
      "[('a', 'DT')]\n",
      "[('capital', 'NN')]\n",
      "[('of', 'IN')]\n",
      "[('Pakistan', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "for i in token_sen2:\n",
    "  word2=nltk.word_tokenize(i)   \n",
    "  tagged2=nltk.pos_tag(word2)\n",
    "  print(tagged2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "55a73c9f-3d9b-4f0e-b15a-73b9bce32197",
   "metadata": {},
   "outputs": [],
   "source": [
    "ChunkGram=r\"\"\"\n",
    "          Chunk:\n",
    "          {<NN>|<NNP>|<NNS>?}\n",
    "          {<VBZ>|<VB>|<VBG>|<VBP>?}\n",
    "          {<IN>}\n",
    "          {<DT>}\n",
    "          {<PRP>}\n",
    "          \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "af63b059-e7de-4491-b231-799bf4c75749",
   "metadata": {},
   "outputs": [],
   "source": [
    "ChunkParser1=nltk.RegexpParser(ChunkGram)\n",
    "chunked1=ChunkParser.parse(tagged1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "356b4d68-7df9-492d-86b8-93fbbdfb9429",
   "metadata": {},
   "outputs": [],
   "source": [
    "ChunkParser2=nltk.RegexpParser(ChunkGram)\n",
    "chunked2=ChunkParser.parse(tagged2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ad6eb058-c022-42b5-96dc-19174a2144fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi ['Turkey']\n"
     ]
    }
   ],
   "source": [
    "chunks1=[]\n",
    "for subtree in chunked1.subtrees(filter=lambda t:t.label()=='Chunk'):\n",
    "    chunk1 =''\n",
    "    for leave in subtree.leaves():\n",
    "      chunk1+=leave[0]\n",
    "      chunks1.append(chunk1.strip()) \n",
    "print(\"hi\",chunks1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "709b6147-d8b5-497a-94cb-0c25c51d056c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi ['Pakistan']\n"
     ]
    }
   ],
   "source": [
    "chunks2=[]\n",
    "for subtree in chunked2.subtrees(filter=lambda t:t.label()=='Chunk'):\n",
    "    chunk2 = \" \"\n",
    "    for leave in subtree.leaves():\n",
    "      chunk2+=leave[0]+' '\n",
    "      chunks2.append(chunk2.strip())\n",
    "print(\"hi\",chunks2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "38a63bd1-7cbd-4079-8d7a-f1d9238687e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From Historic: [0.4444444444444444, Synset('phosphorus.n.01')]\n"
     ]
    }
   ],
   "source": [
    "hi=[]\n",
    "for word1 in chunks1:\n",
    "    for word2 in chunks2:\n",
    "        wordfromlist1 = wordnet.synsets(word1[0])\n",
    "        wordfromlist2 = wordnet.synsets(word2[0])\n",
    "        if wordfromlist1 and wordfromlist2:\n",
    "            s = wordfromlist1[0].wup_similarity(wordfromlist2[0])\n",
    "            hi.append(s)\n",
    "            hi.append(wordfromlist2[0])\n",
    "print(\"From Historic:\",hi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dabe64be-8de2-4f1f-bc4d-4594e905f9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From place: [0.4444444444444444, Synset('thymine.n.01')]\n"
     ]
    }
   ],
   "source": [
    "p=[]\n",
    "for word1 in chunks2:\n",
    "    for word2 in chunks1:\n",
    "        wordfromlist1 = wordnet.synsets(word1[0])\n",
    "        wordfromlist2 = wordnet.synsets(word2[0])\n",
    "        if wordfromlist1 and wordfromlist2:\n",
    "            s = wordfromlist1[0].wup_similarity(wordfromlist2[0])\n",
    "            p.append(s)\n",
    "            p.append(wordfromlist2[0])\n",
    "print(\"From place:\",p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
